import os

# [ì¤‘ìš”] ê·¸ë˜í”½ ì¹´ë“œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì¶©ëŒ ë°©ì§€ (WinError 1114 í•´ê²°ìš©)
os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, random_split
from torchvision import datasets, transforms, models
from torch.optim.lr_scheduler import ReduceLROnPlateau  # í•™ìŠµë¥  ì¡°ì ˆê¸° ì¶”ê°€
import time

# ==========================================
# [ì„¤ì • ì˜ì—­]
# ==========================================
DATA_DIR = './dataset'
SAVE_PATH = 'waste_model.pth'
BATCH_SIZE = 32
LEARNING_RATE = 0.001
EPOCHS = 20  # ì¡°ê¸° ì¢…ë£Œ(Early Stopping)ë¥¼ ìœ„í•´ ë„‰ë„‰í•˜ê²Œ 20ìœ¼ë¡œ ì„¤ì •
NUM_CLASSES = 5


# ==========================================


def train_model():
    print(f" AI í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤! (ì„¤ì •: {EPOCHS} ì—í­, Early Stopping í™œì„±í™”)")
    print(" ì´ë²ˆ í•™ìŠµì€ ì¡°ëª…/ë°°ê²½ ë³€í™”ì— ëœ ë¯¼ê°í•˜ë„ë¡ ë°ì´í„° ì¦ê°•ì´ ê°•í™”ë©ë‹ˆë‹¤.")

    # 1. ì¥ì¹˜ ì„¤ì •
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ğŸ’» ì‚¬ìš©í•  ì¥ì¹˜: {device}")

    # 2. ì´ë¯¸ì§€ ì „ì²˜ë¦¬ (ë°ì´í„° ì¦ê°• ê°•í™”)
    # ------------------------------------------------------------------
    # â­ [í•µì‹¬ ìˆ˜ì •] train_transform: ColorJitter ê°•ë„ë¥¼ ë†’ì—¬ ë°°ê²½ ì˜ì¡´ì„±ì„ ì¤„ì„
    train_transform = transforms.Compose([
        transforms.Resize((224, 224)),
        # ë°ê¸°, ëŒ€ë¹„, ì±„ë„ë¥¼ ë¬´ì‘ìœ„ë¡œ 30%ê¹Œì§€ ë³€í™”ì‹œì¼œ ì¡°ëª… í™˜ê²½ ë‹¤ì–‘í™”
        transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1),
        transforms.RandomHorizontalFlip(),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ])

    # val_transform: ê²€ì¦ ì‹œì—ëŠ” ì¦ê°• ì—†ì´ ê¸°ë³¸ ì „ì²˜ë¦¬ë§Œ ìˆ˜í–‰
    val_transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ])
    # ------------------------------------------------------------------

    # 3. ë°ì´í„°ì…‹ ë° ë°ì´í„° ë¡œë” ì¤€ë¹„
    if not os.path.exists(DATA_DIR):
        print(f" ì˜¤ë¥˜: ë°ì´í„° í´ë” '{DATA_DIR}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í´ë” ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
        return

    full_dataset = datasets.ImageFolder(root=DATA_DIR, transform=train_transform)

    # ë°ì´í„° ë¶„í•  (í›ˆë ¨ 80%, ê²€ì¦ 20%)
    train_size = int(0.8 * len(full_dataset))
    val_size = len(full_dataset) - train_size
    train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])

    # ê²€ì¦ ë°ì´í„°ì…‹ì—ë§Œ ê²€ì¦ìš© ì „ì²˜ë¦¬ ì ìš© ( train_transformì„ val_transformìœ¼ë¡œ êµì²´)
    val_dataset.dataset.transform = val_transform

    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)
    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)

    print(f" ë°ì´í„°ì…‹ ì¤€ë¹„ ì™„ë£Œ: í›ˆë ¨ {len(train_dataset)}ê°œ, ê²€ì¦ {len(val_dataset)}ê°œ")
    print(f"í´ë˜ìŠ¤ ëª©ë¡: {full_dataset.classes}")

    # 4. ëª¨ë¸ ì„¤ì • (EfficientNet B0 ì „ì´ í•™ìŠµ)
    # ìµœì‹  ê°€ì¤‘ì¹˜ ì‚¬ìš©
    try:
        model = models.efficientnet_b0(weights=models.EfficientNet_B0_Weights.DEFAULT)
    except:
        # ìµœì‹  ê°€ì¤‘ì¹˜ ë¡œë“œê°€ ì‹¤íŒ¨í•˜ë©´ ì´ì „ ë°©ì‹ ì‚¬ìš© (ì´ì „ ì½”ë“œë¥¼ ê¸°ë°˜ìœ¼ë¡œ í˜¸í™˜ì„± ìœ ì§€)
        model = models.efficientnet_b0(weights='EfficientNet_B0_Weights.IMAGENET1K_V1')

    # ë¶„ë¥˜ê¸° ë ˆì´ì–´ êµì²´
    num_ftrs = model.classifier[1].in_features
    model.classifier[1] = nn.Linear(num_ftrs, NUM_CLASSES)
    model.to(device)

    # 5. ì†ì‹¤ í•¨ìˆ˜ ë° ì˜µí‹°ë§ˆì´ì € ì„¤ì •
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)

    # í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬ (ê²€ì¦ ì •í™•ë„ê°€ ì •ì²´ë˜ë©´ í•™ìŠµë¥  ê°ì†Œ)
    scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.1, patience=3)

    # 6. ëª¨ë¸ í•™ìŠµ
    best_acc = 0.0
    start_time = time.time()

    for epoch in range(EPOCHS):
        print(f"\n--- Epoch {epoch + 1}/{EPOCHS} ---")

        # [í›ˆë ¨ ëª¨ë“œ]
        model.train()
        running_loss = 0.0
        correct = 0
        total = 0

        for i, (inputs, labels) in enumerate(train_loader):
            inputs, labels = inputs.to(device), labels.to(device)

            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            running_loss += loss.item()
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

        train_acc = 100 * correct / total
        print(f"   [í›ˆë ¨] ì˜¤ì°¨: {running_loss / len(train_loader):.4f}, ì •í™•ë„: {train_acc:.2f}%")

        # [ê²€ì¦ ëª¨ë“œ] (ìµœê³  ì„±ëŠ¥ ì €ì¥ ë¡œì§)
        model.eval()
        val_correct = 0
        val_total = 0
        with torch.no_grad():
            for inputs, labels in val_loader:
                inputs, labels = inputs.to(device), labels.to(device)
                outputs = model(inputs)
                _, predicted = torch.max(outputs.data, 1)
                val_total += labels.size(0)
                val_correct += (predicted == labels).sum().item()

        val_acc = 100 * val_correct / val_total
        print(f"   [ê²€ì¦] ì •í™•ë„: {val_acc:.2f}%")

        # í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬ ì—…ë°ì´íŠ¸
        scheduler.step(val_acc)

        #  ì¡°ê¸° ì¢…ë£Œ ë¡œì§: ìµœê³  ì •í™•ë„ ê°±ì‹  ì‹œ ëª¨ë¸ ì €ì¥
        if val_acc > best_acc:
            best_acc = val_acc
            print(f"    ìµœê³  ì •í™•ë„ ê°±ì‹ ! ëª¨ë¸ ì €ì¥ ({best_acc:.2f}%) ")
            torch.save(model.state_dict(), SAVE_PATH)

        # ì¡°ê¸° ì¢…ë£Œ ì¡°ê±´ (í•™ìŠµë¥ ì´ ë„ˆë¬´ ë‚®ì•„ì§€ë©´ ì¢…ë£Œ)
        if optimizer.param_groups[0]['lr'] < 1e-6:  # í•™ìŠµë¥ ì´ 1e-6ë³´ë‹¤ ë‚®ì•„ì§€ë©´
            print(" í•™ìŠµë¥ ì´ ìµœì €ì¹˜ì— ë„ë‹¬í•˜ì—¬ í•™ìŠµì„ ì¡°ê¸° ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break

    end_time = time.time()
    print(f"\n--- í•™ìŠµ ì¢…ë£Œ ---")
    print(f"ìµœê³  ê²€ì¦ ì •í™•ë„: {best_acc:.2f}%")
    print(f"ì´ ì†Œìš” ì‹œê°„: {end_time - start_time:.2f}ì´ˆ")


if __name__ == '__main__':
    train_model()